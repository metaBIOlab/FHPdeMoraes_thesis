---
title: "Relatorio PRJ Girificacao"
author: "Fernanda Hansen Pacheco de Moraes"
date: "11 de dezembro de 2018"
output:
  html_document: 
    df_print: kable
    fig_caption: yes
    fig_width: 8
    number_sections: yes
    theme: paper
    toc: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	warning = FALSE,
	messAge = FALSE
)
```

```{r preparo, warning=FALSE, include=FALSE, messAge=FALSE}
# PREPARO

# define a area de trabalho
# setwd("Z:/PRJ1513_GIRIFICACAO_CONTROLES/03_PROCS/STATISTICS/V6/")# se no idor
# setwd("E:/idor/Gyrification/") # se no pendrive
# setwd("C:/Users/fernanda.hansen/Desktop/aging/V6/") # se no pendrive
 setwd("C:/Users/ferna/Documents/idor/Gyrification/RRRRRR/aging/V6/") # se no computador

# carrega os pacotes utilizados
source("01-call_packages.R")

source("02-funcoes.R")
# onde estao os arquivos de resultados:
#path_yujiangscript <- str_c("C:/Users/ferna/Documents/idor/Gyrification/data/resultados")

alpha_beta <- str_c("C:/Users/ferna/Documents/idor/Gyrification/data/resultados/")
  
# path_lobes <- str_c("Z:/PRJ1513_GIRIFICACAO_CONTROLES/03_PROCS/PROC_DATA/STATS/LobesScaling/")
path_lobes <- str_c("C:/Users/ferna/Documents/idor/Gyrification/STATS/")
 
# path_sessions <- str_c("Z:/PRJ1513_GIRIFICACAO_CONTROLES/03_PROCS/RAW_DATA/BIDS/")
 path_sessions <- str_c("C:/Users/ferna/Documents/idor/Gyrification/Processamento_longitudinal/V6/data/")

# importa os resultados
source("03-import_files.R") # arquivos dos sujeitos

# organiza os arquivos
source("04-organiza.R")

dados_raw <- dados

verificar_lGI_zero <- filter(dados, ExposedArea == 0 | is.na(localGI))
write.csv(verificar_lGI_zero, "verificar_lGI_zero.csv")

# retira os outros diagnosticos e deixa so quem tem machine definido
dados_all <- dados %>% filter(
    Diagnostic == "CONTROLE" |
      Diagnostic == "CCL" |
      Diagnostic == "ALZ", !is.na(logAvgThickness), ExposedArea != 0 | !is.na(localGI), !is.infinite(logExposedArea)) %>% 
  droplevels()

dados_excluidos <- anti_join(dados_raw, dados_all)
write.csv(dados_excluidos, "dados_excluidos.csv")

dados <- dados_all

Age.cor = 25

# Prepara os dados para analise
source("05-analises_prep.R")

dados$Diagnostic <- factor(dados$Diagnostic, levels = c("ALZ", "CCL","CONTROLE"))
dados$Diagnostic <- revalue(dados$Diagnostic, c("CONTROLE"="CTL", "ALZ"="AD", "CCL" = "MCI"))

dados <- filter(dados, machine == "Philips-Achieva")

# separa em arquivo de hemisferios e de lobos

dados_lobos_v1 <- unique(filter(dados, ROI == "F" | ROI == "T" | ROI == "O" | ROI == "P", Session == 1, !is.na(K_age_decay), SUBJ != "SUBJ211", SUBJ != "SUBJ223")) %>% droplevels()

dados_v1 <- filter(dados, ROI == "F" | ROI == "T" | ROI == "O" | ROI == "P" | ROI == "hemisphere", Session == 1) %>% droplevels()

dados_hemi_v1 <- unique(filter(dados, ROI == "hemisphere", Session == 1, !is.na(K_age_decay)))

write.csv(dados_v1,"dados_v1.csv")
```

# Metodo

```{r}

paste("N subjects LGI = 0 = ", n_distinct(filter(verificar_lGI_zero, machine == "Philips-Achieva", !is.na(Age), Diagnostic =="CCL" | Diagnostic == "ALZ" | Diagnostic == "CONTROLE", Session == "1")$SUBJ))

unique(filter(verificar_lGI_zero, machine == "Philips-Achieva", !is.na(Age), Diagnostic =="CCL" | Diagnostic == "ALZ" | Diagnostic == "CONTROLE", Session == "1")$SUBJ)
filter(verificar_lGI_zero, machine == "Philips-Achieva", !is.na(Age), Diagnostic =="CCL" | Diagnostic == "ALZ" | Diagnostic == "CONTROLE", Session == "1") %>% kable() %>% kable_styling()
```

# Distribuição da Sample

## Escolaridade mínima 8 anos

```{r}

ggplot(data = dados_hemi_v1, aes(x = Diagnostic, y = ESC, color = Diagnostic, fill = Diagnostic, alpha = 0.4)) + 
  geom_boxplot() + theme_pubr() +
  stat_compare_means(method = "anova")

paste("Escolaridade minima = ", min(dados_hemi_v1$ESC), "N de sujeitos com escolaridade = 4 anos, = ", n_distinct(filter(dados_hemi_v1, ESC == 4)$SUBJ))

paste("N de sujeitos com escolaridade = 5 anos, = ", n_distinct(filter(dados_hemi_v1, ESC == 5)$SUBJ))

paste("N de sujeitos com escolaridade = 7 anos, = ", n_distinct(filter(dados_hemi_v1, ESC == 7)$SUBJ))

dados <- filter(dados, ESC == 8 | ESC > 8)

dados_lobos_v1 <- unique(filter(dados, ROI == "F" | ROI == "T" | ROI == "O" | ROI == "P", Session == 1, !is.na(K_age_decay), SUBJ != "SUBJ211", SUBJ != "SUBJ223")) %>% droplevels()

dados_v1 <- filter(dados, ROI == "F" | ROI == "T" | ROI == "O" | ROI == "P" | ROI == "hemisphere", Session == 1) %>% droplevels()

dados_hemi_v1 <- unique(filter(dados, ROI == "hemisphere", Session == 1, !is.na(K_age_decay)))

```


## Numero de sujeitos

### Todos da lista
```{r N sujeitos, echo=FALSE, warning=FALSE, message=FALSE}
summary_dados_all <- filter(dados_raw, Session == 1) %>%
  group_by(machine, Diagnostic) %>%
  summarise(
  N = n_distinct(SUBJ),
  Mean = mean(Age),
  Max = max(Age),
  Min = min(Age),
  Median = median(Age),
  Std = sd(Age)
  )
  summary_dados_all %>% kable(digits = 2) %>% kable_styling()
  
paste("N sujeitos = ",n_distinct(filter(dados_raw, Session == 1)$SUBJ))  
paste("N sujeitos Philips = ",n_distinct(filter(dados_raw, Session == 1, machine == "Philips-Achieva")$SUBJ))
paste("N sujeitos Philips CTL, MCI e AD = ",n_distinct(filter(dados_raw, Session == 1, machine == "Philips-Achieva", Diagnostic =="CCL" | Diagnostic == "ALZ" | Diagnostic == "CONTROLE")$SUBJ))


# SUBJ_raw <- unique( dplyr::select(filter(dados_raw, Session == 1, machine == "Philips-Achieva", Diagnostic =="CCL" | Diagnostic == "ALZ" | Diagnostic == "CONTROLE"), c(SUBJ)))
# SUBJ_v1 <- unique( dplyr::select(dados_hemi_v1, c(SUBJ)))
# anti_join(SUBJ_raw, SUBJ_v1)
# filter(dados_raw, Session == 1, SUBJ == "SUBJ081"| SUBJ == "SUBJ139" | SUBJ == "SUBJ211" | SUBJ == "SUBJ223") %>% kable(digits =2) %>% kable_styling()
# filter(dados, Session == 1, SUBJ == "SUBJ081"| SUBJ == "SUBJ139" | SUBJ == "SUBJ211" | SUBJ == "SUBJ223") %>% kable(digits =2) %>% kable_styling()

```

### Todos processados pelo FreeSurfer com sucesso
```{r N sujeitos FS, echo=FALSE, warning=FALSE, message=FALSE}
summary_dados_all <- filter(dados_all, Session == 1) %>%
  group_by(Diagnostic, machine) %>%
  summarise(
  N = n_distinct(SUBJ),
  Mean = mean(Age),
  Max = max(Age),
  Min = min(Age),
  Median = median(Age),
  Std = sd(Age)
  )
  summary_dados_all %>% kable(digits = 2) %>% kable_styling()
  
  paste("N sujeitos = ",n_distinct(filter(dados_all, Session == 1)$SUBJ))  
```

### Somente Philips

```{r N sujeitos Philips, echo=FALSE, message=FALSE, warning=FALSE}
dados_hemi_v1 %>%
  group_by(Diagnostic) %>%
  summarise(
    N = n_distinct(SUBJ),
    age = paste(signif(mean(Age), 2), "±", signif(sd(Age), 2)),
    age_range = paste(signif(min(Age), 2), "; ", signif(max(Age), 2)),
    ESC = paste(signif(mean(ESC), 2), "±", signif(sd(ESC), 2)),
    COGNITIVE_INDEX = paste(signif(mean(COGNITIVE_INDEX), 2), "±", signif(sd(COGNITIVE_INDEX), 2)),
    TAU = paste(signif(mean(TAU), 2), "±", signif(sd(TAU), 2)),
    AB1_40 = paste(signif(mean(`AB1-40`), 2), "±", signif(sd(`AB1-40`), 2)),
    AB1_42 = paste(signif(mean(`AB1-42`), 2), "±", signif(sd(`AB1-42`), 2)),
    Lipoxin = paste(signif(mean(Lipoxina), 2), "±", signif(sd(Lipoxina), 2)),
    AvgT =  paste(signif(mean(AvgThickness), 2), "±", signif(sd(AvgThickness), 2)),
    AT =  paste(signif(mean(TotalArea), 2), "±", signif(sd(TotalArea), 2)),
    AE =  paste(signif(mean(ExposedArea), 2), "±", signif(sd(ExposedArea), 2)),
    k =  paste(signif(mean(k), 2), "±", signif(sd(k), 2)),
    K =  paste(signif(mean(K), 2), "±", signif(sd(K), 2)),
    S =  paste(signif(mean(S), 2), "±", signif(sd(S), 2)),
    I =  paste(signif(mean(I), 2), "±", signif(sd(I), 2))
  ) %>% kable(digits = 2) %>% kable_styling()

dados_hemi_v1 %>%
  group_by(Gender, Diagnostic) %>%
  summarise(
    N = n_distinct(SUBJ),
    age = paste(signif(mean(Age), 2), "±", signif(sd(Age), 2)),
    age_range = paste(signif(min(Age), 2), "; ", signif(max(Age), 2)),
    ESC = paste(signif(mean(ESC), 2), "±", signif(sd(ESC), 2)),
    COGNITIVE_INDEX = paste(signif(mean(COGNITIVE_INDEX), 2), "±", signif(sd(COGNITIVE_INDEX), 2)),
    TAU = paste(signif(mean(TAU), 2), "±", signif(sd(TAU), 2)),
    AB1_40 = paste(signif(mean(`AB1-40`), 2), "±", signif(sd(`AB1-40`), 2)),
    AB1_42 = paste(signif(mean(`AB1-42`), 2), "±", signif(sd(`AB1-42`), 2)),
    Lipoxin = paste(signif(mean(Lipoxina), 2), "±", signif(sd(Lipoxina), 2)),
    AvgT =  paste(signif(mean(AvgThickness), 2), "±", signif(sd(AvgThickness), 2)),
    AT =  paste(signif(mean(TotalArea), 2), "±", signif(sd(TotalArea), 2)),
    AE =  paste(signif(mean(ExposedArea), 2), "±", signif(sd(ExposedArea), 2)),
    k =  paste(signif(mean(k), 2), "±", signif(sd(k), 2)),
    K =  paste(signif(mean(K), 2), "±", signif(sd(K), 2)),
    S =  paste(signif(mean(S), 2), "±", signif(sd(S), 2)),
    I =  paste(signif(mean(I), 2), "±", signif(sd(I), 2))
  ) %>% kable(digits = 2) %>% kable_styling()

lm_Age <- lm(Age ~ Diagnostic, data = dados_hemi_v1)
anova(lm_Age)
Age_diag_tk <- TukeyHSD(aov(Age ~ Diagnostic, data = dados_hemi_v1))
Age_diag_tk

lm_esc <- lm(ESC ~ Diagnostic, data = dados_hemi_v1)
anova(lm_esc)
ESC_diag_TK <- TukeyHSD(aov(ESC ~ Diagnostic, data = dados_hemi_v1))
ESC_diag_TK

lm_COGNITIVE_INDEX <-
  lm(COGNITIVE_INDEX ~ Diagnostic, data = dados_hemi_v1)
anova(lm_COGNITIVE_INDEX)
COGINDX_diag_TK <-
  TukeyHSD(aov(COGNITIVE_INDEX ~ Diagnostic, data = dados_hemi_v1))
COGINDX_diag_TK

paste("N sujeitos = ", n_distinct(dados_hemi_v1$SUBJ))
paste("N sujeitos lobos = ", n_distinct(dados_lobos_v1$SUBJ))

SUBJ_hemi <- unique(dplyr::select(dados_hemi_v1, c(SUBJ)))
SUBJ_lobos <- unique(dplyr::select(dados_lobos_v1, c(SUBJ)))
anti_join(SUBJ_lobos, SUBJ_hemi)

```

### Sujeitos excluídos

```{r N sujeitos excluidos, echo=FALSE, message=FALSE, warning=FALSE}

excl <-
  dados_excluidos %>% dplyr::select(c(SUBJ, Age, Diagnostic, machine, ROI, localGI)) %>% filter(ROI == "hemisphere") %>% distinct(SUBJ, .keep_all = TRUE)
  
  excl %>% kable(digits = 2) %>% kable_styling()

  paste("N sujeitos = ",n_distinct(excl$SUBJ))  
```

### lista sujeitos
```{r subjs, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
dados_hemi_v1 %>% dplyr::select(SUBJ, acq_date, Age, Gender, Diagnostic) %>% unique() %>% kable(digits = 2) %>% kable_styling()
```


```{r distribuicao da Sample}
ggplot(data = dados_hemi_v1, aes(x = Diagnostic, y = Age, color = Diagnostic, fill = Diagnostic, alpha = 0.4)) + 
  geom_boxplot() + theme_pubr() +
  stat_compare_means(method = "anova")


ggplot(data = dados_hemi_v1, aes(x = Diagnostic, y = K, color = Gender, fill = Gender, alpha = 0.4)) +
  geom_boxplot() + theme_pubr() +
  stat_compare_means(method = "anova")

```

```{r}
dados_hemi_v1 %>%
group_by(Diagnostic)  %>%
summarise(
N = n_distinct(SUBJ),
    age = paste(signif(mean(Age), 2), "±", signif(sd(Age), 2)),
    age_range = paste(signif(min(Age), 2), "; ", signif(max(Age), 2)),
    ESC = paste(signif(mean(ESC), 2), "±", signif(sd(ESC), 2)),
    COGNITIVE_INDEX = paste(signif(mean(COGNITIVE_INDEX), 2), "±", signif(sd(COGNITIVE_INDEX), 2)),
    TAU = paste(signif(mean(TAU), 2), "±", signif(sd(TAU), 2)),
    AB1_40 = paste(signif(mean(`AB1-40`), 2), "±", signif(sd(`AB1-40`), 2)),
    AB1_42 = paste(signif(mean(`AB1-42`), 2), "±", signif(sd(`AB1-42`), 2)),
    Lipoxin = paste(signif(mean(Lipoxina), 2), "±", signif(sd(Lipoxina), 2)),
    AvgT =  paste(signif(mean(AvgThickness), 2), "±", signif(sd(AvgThickness), 2)),
    AT =  paste(signif(mean(TotalArea), 2), "±", signif(sd(TotalArea), 2)),
    AE =  paste(signif(mean(ExposedArea), 2), "±", signif(sd(ExposedArea), 2)),
    k =  paste(signif(mean(k), 2), "±", signif(sd(k), 2)),
    K =  paste(signif(mean(K), 2), "±", signif(sd(K), 2)),
    S =  paste(signif(mean(S), 2), "±", signif(sd(S), 2)),
    I =  paste(signif(mean(I), 2), "±", signif(sd(I), 2))) %>% kable(digits = 2) %>% kable_styling()

```

# DIAGNOSTIC PREDICTION ----

## modelo multinomial

```{r glm, echo=TRUE, messAge=FALSE, warning=FALSE}
require(foreign)
require(nnet)
require(reshape2)
library(stargazer)
require(betareg)

#Diagnostic <- factor(dados_hemi_v1$Diagnostic, levels = c("AD", "MCI","CTL"))
dados_hemi_v1$Diagnostic <- relevel(dados_hemi_v1$Diagnostic, "CTL")

set.seed(0)

N_diag <- dados_hemi_v1 %>% dplyr::select(SUBJ, Diagnostic) %>% unique() %>% group_by(Diagnostic) %>% summarise(n_DIAG = n_distinct(SUBJ))

dados_hemi_v1_filter <- dados_hemi_v1 %>% dplyr::select(SUBJ, Diagnostic) %>% unique()

N_CTL <- as.double(floor(N_diag[1,2]*0.8))
N_CCL <- as.double(round(N_diag[3,2]*0.8))
N_ALZ <- as.double(round(N_diag[2,2]*0.8))

test.samples <- c(sample(which(dados_hemi_v1_filter$Diagnostic == "AD"), N_ALZ), sample(which(dados_hemi_v1_filter$Diagnostic == "CTL"), N_CTL), sample(which(dados_hemi_v1_filter$Diagnostic == "MCI"), N_CCL))
subj.training <- as_tibble(dados_hemi_v1_filter[-test.samples, ]$SUBJ)

colnames(subj.training) <- c("SUBJ")

# filter(dados_hemi_v1, SUBJ == subj.training)

train.data <- anti_join(dados_hemi_v1, subj.training)
test.data <- semi_join(dados_hemi_v1, subj.training)

#train.data  <- dados_hemi_v1[-test.samples, ]
#test.data <- dados_hemi_v1[test.samples, ]

caret::featurePlot(x = dados_hemi_v1[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = dados_hemi_v1$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

caret::featurePlot(x = dados_hemi_v1[, c("K", "I", "S")], y = dados_hemi_v1$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(3,1))
caret::featurePlot(x = dados_hemi_v1[, c("K_age_decay", "I_age_decay", "S_age_decay")], y = dados_hemi_v1$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(3,1))

print(n_distinct(dados_hemi_v1$SUBJ))
print(n_distinct(train.data$SUBJ))
print(n_distinct(test.data$SUBJ))


# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = K, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(K))$SUBJ))) 

#aov1 <- aov(K ~ Diagnostic, dados_hemi_v1)
#TukeyHSD(aov1)

# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = K_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova")  + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(K_age_decay))$SUBJ))) 

#aov2 <- aov(K_age_decay ~ Diagnostic, dados_hemi_v1)
#TukeyHSD(aov2)

#ggplot(dados_hemi_v1, aes(x = Diagnostic, y = logAvgThickness, color = Diagnostic, fill = Diagnostic)) +
#geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
#theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(logAvgThickness))$SUBJ))) 

#aov3 <- aov(logAvgThickness ~ Diagnostic, dados_hemi_v1)
#TukeyHSD(aov3)

# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = logAvgThickness_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(logAvgThickness_age_decay))$SUBJ))) 


#aov4 <- aov(logAvgThickness_age_decay ~ Diagnostic, dados_hemi_v1)
#TukeyHSD(aov4)

# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = I, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(logAvgThickness_age_decay))$SUBJ))) 
# 
# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = I_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(logAvgThickness_age_decay))$SUBJ))) 
# 
# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = S, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(logAvgThickness_age_decay))$SUBJ))) 
# 
# ggplot(dados_hemi_v1, aes(x = Diagnostic, y = S_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_hemi_v1, !is.na(logAvgThickness_age_decay))$SUBJ))) 


caret::featurePlot(x = train.data[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

caret::featurePlot(x = train.data[, c("K", "K_age_decay", "I", "I_age_decay", "S", "S_age_decay")], y = train.data$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(3,2))

multinom1 <- multinom(Diagnostic ~ K + Age, data = train.data)
multinom2 <- multinom(Diagnostic ~ logAvgThickness + Age, data = train.data)

multinom10 <- multinom(Diagnostic ~ S + Age, data = train.data)
multinom11 <- multinom(Diagnostic ~ I + Age, data = train.data)

multinom0 <- multinom(Diagnostic ~ K + Age + ESC, data = train.data)
multinom0_2 <- multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data)

multinom_Gender1 <- multinom(Diagnostic ~ K + Age + Gender , data = train.data)
multinom_Gender2 <- multinom(Diagnostic ~ logAvgThickness + Age + Gender , data = train.data)

multinom0_0 <- multinom(Diagnostic ~ K + S + I + Age, data = train.data)


# anova(multinom2, multinom1, test = "Chisq")
# anova(multinom0, multinom1, test = "Chisq")
# 
# anova(multinom0_0, multinom1, test = "Chisq")

## da estatistica ##
 
m.multi.nova1 <-
  multinom(Diagnostic ~ K + Age, data = train.data)
  stargazer(m.multi.nova1, type = "text")

  z1 <-
    summary(m.multi.nova1)$coefficients / summary(m.multi.nova1)$standard.errors
    p1 <- (1 - pnorm(abs(z1), 0, 1)) * 2
    t(p1)

#Para facilitar a interpreta??o:
coef.multi1 = exp(coef(m.multi.nova1))
t(coef.multi1)
#Previsoes
predicted.classes.multi.nova1 <- m.multi.nova1 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova1 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova1, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova1),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova2 <-
  multinom(Diagnostic ~ logAvgThickness + Age, data = train.data)
  stargazer(m.multi.nova2, type = "text")

  z2 <-
    summary(m.multi.nova2)$coefficients / summary(m.multi.nova2)$standard.errors
    p2 <- (1 - pnorm(abs(z2), 0, 1)) * 2
    t(p2)

#Para facilitar a interpreta??o:
coef.multi2 = exp(coef(m.multi.nova2))
t(coef.multi2)
#Previsoes
predicted.classes.multi.nova2 <- m.multi.nova2 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova2 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova2, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova2),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova10 <-
  multinom(Diagnostic ~ I + Age, data = train.data)
  stargazer(m.multi.nova10, type = "text")

  z10 <-
    summary(m.multi.nova10)$coefficients / summary(m.multi.nova10)$standard.errors
    p10 <- (1 - pnorm(abs(z10), 0, 1)) * 2
    t(p10)

#Para facilitar a interpreta??o:
coef.multi10 = exp(coef(m.multi.nova10))
t(coef.multi10)
#Previsoes
predicted.classes.multi.nova10 <- m.multi.nova10 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova10 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova10, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova10),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova11 <-
  multinom(Diagnostic ~ S + Age, data = train.data)
  stargazer(m.multi.nova10, type = "text")

  z11 <-
    summary(m.multi.nova11)$coefficients / summary(m.multi.nova11)$standard.errors
    p11 <- (1 - pnorm(abs(z11), 0, 1)) * 2
    t(p11)

#Para facilitar a interpreta??o:
coef.multi11 = exp(coef(m.multi.nova11))
t(coef.multi11)
#Previsoes
predicted.classes.multi.nova11 <- m.multi.nova11 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova11 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova11, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova11),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0 <-
  multinom(Diagnostic ~ K + Age + ESC, data = train.data)
  stargazer(m.multi.nova0, type = "text")
  
  z0 <-
    summary(m.multi.nova0)$coefficients / summary(m.multi.nova0)$standard.errors
    p0 <- (1 - pnorm(abs(z0), 0, 1)) * 2
    t(p0)

#Para facilitar a interpreta??o:
coef.multi0 = exp(coef(m.multi.nova0))
t(coef.multi0)
#Previsoes
predicted.classes.multi.nova0 <- m.multi.nova0 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova0, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_2 <-
  multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data)
  stargazer(m.multi.nova0_2, type = "text")

  z0_2 <-
    summary(m.multi.nova0_2)$coefficients / summary(m.multi.nova0_2)$standard.errors
    p0_2 <- (1 - pnorm(abs(z0_2), 0, 1)) * 2
    t(p0_2)

#Para facilitar a interpreta??o:
coef.multi0_2 = exp(coef(m.multi.nova0_2))
t(coef.multi0_2)
#Previsoes
predicted.classes.multi.nova0_2 <- m.multi.nova0_2 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_2 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova0_2, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_2),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_0 <-
  multinom(Diagnostic ~ K + I + S + Age, data = train.data)
  stargazer(m.multi.nova0_0, type = "text")

  z0_0 <-
    (summary(m.multi.nova0_0)$coefficients)/(summary(m.multi.nova0_0)$standard.errors)
    p0_0 <- (1 - pnorm(abs(z0_0), 0, 1)) * 2
    t(p0_0)

#Para facilitar a interpreta??o:
coef.multi0_0 = exp(coef(m.multi.nova0_0))
t(coef.multi0_0)
#Previsoes
predicted.classes.multi.nova0_0 <- m.multi.nova0_0 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_0 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0_0, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_0),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "0_0-Specificity (false positive rate)"
  )


m.multi.nova.Gender1 <-
  multinom(Diagnostic ~ K + Age + Gender, data = train.data)
  stargazer(m.multi.nova.Gender1, type = "text")

  z.Gender1 <-
    summary(m.multi.nova.Gender1)$coefficients / summary(m.multi.nova.Gender1)$standard.errors
    p.Gender1 <- (1 - pnorm(abs(z.Gender1), 0, 1)) * 2
    t(p.Gender1)

#Para facilitar a interpreta??o:
coef.multi.Gender1 = exp(coef(m.multi.nova.Gender1))
t(coef.multi.Gender1)
#Previsoes
predicted.classes.multi.nova.Gender1 <- m.multi.nova.Gender1 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender1 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender1, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender1),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova.Gender2 <-
  multinom(Diagnostic ~ logAvgThickness_age_decay + Age + Gender, data = train.data)
  stargazer(m.multi.nova.Gender2, type = "text")

  z.Gender2 <-
    summary(m.multi.nova.Gender2)$coefficients / summary(m.multi.nova.Gender2)$standard.errors
    p.Gender2 <- (1 - pnorm(abs(z.Gender2), 0, 1)) * 2
    t(p.Gender2)

#Para facilitar a interpreta??o:
coef.multi.Gender2 = exp(coef(m.multi.nova.Gender2))
t(coef.multi.Gender2)
#Previsoes
predicted.classes.multi.nova.Gender2 <- m.multi.nova.Gender2 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender2 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender2, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender2),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

## modelo multinomial - deAged

```{r glm deAged, echo=TRUE, messAge=FALSE, warning=FALSE}

# caret::featurePlot(x = train.data[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))


multinom4 <- multinom(Diagnostic ~ K_age_decay, data = train.data)
multinom5 <- multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data)

multinom12 <- multinom(Diagnostic ~ I_age_decay + logAvgThickness_age_decay, data = train.data)
multinom13 <- multinom(Diagnostic ~ S_age_decay + logAvgThickness_age_decay, data = train.data)

multinom0_0_0 <- multinom(Diagnostic ~ K_age_decay + I_age_decay + S_age_decay, data = train.data)

# anova(multinom5, multinom4, test = "Chisq")
# 
# anova(multinom0_0_0, multinom4, test = "Chisq")


## da estatistica ##
 
m.multi.nova4 <-
  multinom(Diagnostic ~ K_age_decay, data = train.data)
  stargazer(m.multi.nova4, type = "text")

  z4 <-
    summary(m.multi.nova4)$coefficients / summary(m.multi.nova4)$standard.errors
    p4 <- (1 - pnorm(abs(z4), 0, 1)) * 2
    t(p4)

#Para facilitar a interpreta??o:
coef.multi4 = exp(coef(m.multi.nova4))
t(coef.multi4)
#Previsoes
predicted.classes.multi.nova4 <- m.multi.nova4 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova4 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova4, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova4),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova5 <-
  multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data)
  stargazer(m.multi.nova5, type = "text")

  z5 <-
    summary(m.multi.nova5)$coefficients / summary(m.multi.nova5)$standard.errors
    p5 <- (1 - pnorm(abs(z5), 0, 1)) * 2
    t(p5)

#Para facilitar a interpreta??o:
coef.multi5 = exp(coef(m.multi.nova5))
t(coef.multi5)
#Previsoes
predicted.classes.multi.nova5 <- m.multi.nova5 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova5 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova5, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova5),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova12 <-
  multinom(Diagnostic ~ I_age_decay, data = train.data)
  stargazer(m.multi.nova12, type = "text")

  z12 <-
    summary(m.multi.nova12)$coefficients / summary(m.multi.nova12)$standard.errors
    p12 <- (1 - pnorm(abs(z12), 0, 1)) * 2
    t(p12)

#Para facilitar a interpreta??o:
coef.multi12 = exp(coef(m.multi.nova12))
t(coef.multi12)
#Previsoes
predicted.classes.multi.nova12 <- m.multi.nova12 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova12 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova12, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova12),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova13 <-
  multinom(Diagnostic ~ S_age_decay, data = train.data)
  stargazer(m.multi.nova12, type = "text")

  z13 <-
    summary(m.multi.nova13)$coefficients / summary(m.multi.nova13)$standard.errors
    p13 <- (1 - pnorm(abs(z13), 0, 1)) * 2
    t(p13)

#Para facilitar a interpreta??o:
coef.multi13 = exp(coef(m.multi.nova13))
t(coef.multi13)
#Previsoes
predicted.classes.multi.nova13 <- m.multi.nova13 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova13 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova13, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova13),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )
  

m.multi.nova0_0_0 <-
  multinom(Diagnostic ~ K_age_decay + I_age_decay + S_age_decay, data = train.data)
  stargazer(m.multi.nova0_0_0, type = "text")

  z0_0_0 <-
    summary(m.multi.nova0_0_0)$coefficients / summary(m.multi.nova0_0_0)$standard.errors
    p0_0_0 <- (1 - pnorm(abs(z0_0_0), 0, 1)) * 2
    t(p0_0_0)

#Para facilitar a interpreta??o:
coef.multi0_0_0 = exp(coef(m.multi.nova0_0_0))
t(coef.multi0_0_0)
#Previsoes
predicted.classes.multi.nova0_0_0 <- m.multi.nova0_0_0 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_0_0 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova0_0_0, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_0_0),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )  
  

m.multi.nova.Gender3 <-
  multinom(Diagnostic ~ K_age_decay + Gender, data = train.data)
  stargazer(m.multi.nova.Gender3, type = "text")

  z.Gender3 <-
    summary(m.multi.nova.Gender3)$coefficients / summary(m.multi.nova.Gender3)$standard.errors
    p.Gender3 <- (1 - pnorm(abs(z.Gender3), 0, 1)) * 2
    t(p.Gender3)

#Para facilitar a interpreta??o:
coef.multi.Gender3 = exp(coef(m.multi.nova.Gender3))
t(coef.multi.Gender3)
#Previsoes
predicted.classes.multi.nova.Gender3 <- m.multi.nova.Gender3 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender3 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender3, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender3),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova.Gender4 <-
  multinom(Diagnostic ~ logAvgThickness + Age + Gender, data = train.data)
  stargazer(m.multi.nova.Gender4, type = "text")

  z.Gender4 <-
    summary(m.multi.nova.Gender4)$coefficients / summary(m.multi.nova.Gender4)$standard.errors
    p.Gender4 <- (1 - pnorm(abs(z.Gender4), 0, 1)) * 2
    t(p.Gender4)

#Para facilitar a interpreta??o:
coef.multi.Gender4 = exp(coef(m.multi.nova.Gender4))
t(coef.multi.Gender4)
#Previsoes
predicted.classes.multi.nova.Gender4 <- m.multi.nova.Gender4 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender4 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender4, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender4),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

## modelo multinomial - lobes

### Lobo temporal

```{r glm temporal lobe, echo=TRUE, messAge=FALSE, warning=FALSE}

#dados_lobos_v1_T$Diagnostic <- factor(dados_lobos_v1_T$Diagnostic, levels = c("AD", "MCI","CTL"))
dados_lobos_v1_T$Diagnostic <- relevel(dados_lobos_v1_T$Diagnostic, "CTL")

# test.samples <- c(sample(which(dados_hemi_v1_filter$Diagnostic == "AD"), N_ALZ), sample(which(dados_hemi_v1_filter$Diagnostic == "CTL"), N_CTL), sample(which(dados_hemi_v1_filter$Diagnostic == "MCI"), N_CCL))
# subj.training <- as_tibble(dados_hemi_v1_filter[-test.samples, $SUBJ)

# colnames(subj.training) <- c("SUBJ")

# filter(dados_lobos_v1_T, SUBJ == subj.training)

train.data_lobes <- anti_join(dados_lobos_v1_T, subj.training)
test.data_lobes <- semi_join(dados_lobos_v1_T, subj.training)

#train.data_lobes  <- dados_lobos_v1_T[-test.samples, ]
#test.data_lobes <- dados_lobos_v1_T[test.samples, ]

caret::featurePlot(x = dados_lobos_v1_T[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = dados_lobos_v1_T$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

print(n_distinct(dados_lobos_v1_T$SUBJ))
print(n_distinct(train.data_lobes$SUBJ))
print(n_distinct(test.data_lobes$SUBJ))


# ggplot(dados_lobos_v1_T, aes(x = Diagnostic, y = K_corrected, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_T, !is.na(K))$SUBJ))) 

#aov1.l <- aov(K ~ Diagnostic, dados_lobos_v1_T)
#aov1.l_TK <-TukeyHSD(aov1.l)
#aov1.l_TK
#plot(aov1.l_TK , las=1 , col="brown")

# ggplot(dados_lobos_v1_T, aes(x = Diagnostic, y = K_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova")  + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_T, !is.na(K_age_decay))$SUBJ))) 

#aov2.l <- aov(K_age_decay ~ Diagnostic, dados_lobos_v1_T)
#aov2.l_TK <-TukeyHSD(aov2.l)
#aov2.l_TK
#plot(aov2.l_TK , las=1 , col="brown")

# ggplot(dados_lobos_v1_T, aes(x = Diagnostic, y = logAvgThickness, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_T, !is.na(logAvgThickness))$SUBJ))) 

#aov3.l <- aov(logAvgThickness ~ Diagnostic, dados_lobos_v1_T)
#TukeyHSD(aov3.l)
#aov3.l_TK <-TukeyHSD(aov3.l)
#aov3.l_TK
#plot(aov3.l_TK , las=1 , col="brown")

# ggplot(dados_lobos_v1_T, aes(x = Diagnostic, y = logAvgThickness_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() + 
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_T, !is.na(logAvgThickness_age_decay))$SUBJ))) 

#aov4.l <- aov(logAvgThickness_age_decay ~ Diagnostic, dados_lobos_v1_T)
#aov4.l_TK <-TukeyHSD(aov4.l)
#aov4.l_TK
#plot(aov4.l_TK , las=1 , col="brown")

caret::featurePlot(x = train.data_lobes[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data_lobes$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

multinom1.l <- multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
multinom2.l <- multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)

multinom4.l <- multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
multinom5.l <- multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)

multinom0.l <- multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
multinom0_2.l <- multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)

## da estatistica ##
 
summary(multinom1.l)
summary(multinom2.l)
summary(multinom4.l)
summary(multinom5.l)
summary(multinom0.l)
summary(multinom0_2.l)

# anova(multinom5, multinom4, test = "Chisq")

## da estatistica ##

m.multi.nova1.l <-
  multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
  stargazer(m.multi.nova1.l, type = "text")

  z1.l <-
    summary(m.multi.nova1.l)$coefficients / summary(m.multi.nova1.l)$standard.errors
    p1.l <- (1 - pnorm(abs(z1.l), 0, 1)) * 2
    t(p1.l)

#Para facilitar a interpreta??o:
coef.multi1.l = exp(coef(m.multi.nova1.l))
t(coef.multi1.l)

#Previsoes
predicted.classes.multi.nova1.l <- m.multi.nova1.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova1.l == test.data_lobes$Diagnostic)

# Summary
cM1.l <- confusionMatrix(predicted.classes.multi.nova1.l, test.data_lobes$Diagnostic)
cM1.l
#cM1.l.t.score <- mutate(cM1.l, Diagnostic = c("AD", "MCI", "CTL"),sensitivity = as.data.frame(cM1.l$byClass[,1]), specificity = as.data.frame(cM1.l$byClass[,2]))

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova1.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)
  stargazer(m.multi.nova2.l, type = "text")

  z2.l <-
    summary(m.multi.nova2.l)$coefficients / summary(m.multi.nova2.l)$standard.errors
    p2.l <- (1 - pnorm(abs(z2.l), 0, 1)) * 2
    t(p2.l)

#Para facilitar a interpreta??o:
coef.multi2.l = exp(coef(m.multi.nova2.l))
t(coef.multi2.l)

#Previsoes
predicted.classes.multi.nova2.l <- m.multi.nova2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )
 
m.multi.nova4.l <-
  multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova4.l, type = "text")

  z4.l <-
    summary(m.multi.nova4.l)$coefficients / summary(m.multi.nova4.l)$standard.errors
    p4.l <- (1 - pnorm(abs(z4.l), 0, 1)) * 2
    t(p4.l)

#Para facilitar a interpreta??o:
coef.multi4.l = exp(coef(m.multi.nova4.l))
t(coef.multi4.l)

#Previsoes
predicted.classes.multi.nova4.l <- m.multi.nova4.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova4.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova4.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova4.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova5.l <-
  multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova5.l, type = "text")

  z5.l <-
    summary(m.multi.nova5.l)$coefficients / summary(m.multi.nova5.l)$standard.errors
    p5.l <- (1 - pnorm(abs(z5.l), 0, 1)) * 2
    t(p5.l)

#Para facilitar a interpreta??o:
coef.multi5.l = exp(coef(m.multi.nova5.l))
t(coef.multi5.l)

#Previsoes
predicted.classes.multi.nova5.l <- m.multi.nova5.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova5.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova5.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova5.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova0.l <-
  multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0.l, type = "text")

  z0.l <-
    summary(m.multi.nova0.l)$coefficients / summary(m.multi.nova0.l)$standard.errors
    p0.l <- (1 - pnorm(abs(z0.l), 0, 1)) * 2
    t(p0.l)

#Para facilitar a interpreta??o:
coef.multi0.l = exp(coef(m.multi.nova0.l))
t(coef.multi0.l)

#Previsoes
predicted.classes.multi.nova0.l <- m.multi.nova0.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0_2.l, type = "text")

  z0_2.l <-
    summary(m.multi.nova0_2.l)$coefficients / summary(m.multi.nova0_2.l)$standard.errors
    p0_2.l <- (1 - pnorm(abs(z0_2.l), 0, 1)) * 2
    t(p0_2.l)

#Para facilitar a interpreta??o:
coef.multi0_2.l = exp(coef(m.multi.nova0_2.l))
t(coef.multi0_2.l)

#Previsoes
predicted.classes.multi.nova0_2.l <- m.multi.nova0_2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0_2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

### Lobo parietal

```{r glm parietal lobe, echo=TRUE, messAge=FALSE, warning=FALSE}

dados_lobos_v1_P$Diagnostic <- factor(dados_lobos_v1_P$Diagnostic, levels = c("AD", "MCI","CTL"))
#dados_lobos_v1_P$Diagnostic <- relevel(dados_lobos_v1_P$Diagnostic, "CTL")

# test.samples <- c(sample(which(dados_hemi_v1_filter$Diagnostic == "AD"), N_ALZ), sample(which(dados_hemi_v1_filter$Diagnostic == "CTL"), N_CTL), sample(which(dados_hemi_v1_filter$Diagnostic == "MCI"), N_CCL))
# subj.training <- as_tibble(dados_hemi_v1_filter[-test.samples, ]$SUBJ)

# colnames(subj.training) <- c("SUBJ")

# filter(dados_lobos_v1_P, SUBJ == subj.training)

train.data_lobes <- anti_join(dados_lobos_v1_P, subj.training)
test.data_lobes <- semi_join(dados_lobos_v1_P, subj.training)

#train.data_lobes  <- dados_lobos_v1_P[-test.samples, ]
#test.data_lobes <- dados_lobos_v1_P[test.samples, ]

caret::featurePlot(x = dados_lobos_v1_P[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = dados_lobos_v1_P$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

print(n_distinct(dados_lobos_v1_P$SUBJ))
print(n_distinct(train.data_lobes$SUBJ))
print(n_distinct(test.data_lobes$SUBJ))


# ggplot(dados_lobos_v1_P, aes(x = Diagnostic, y = K_corrected, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() +
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_P, !is.na(K))$SUBJ)))
# 
# ggplot(dados_lobos_v1_P, aes(x = Diagnostic, y = K_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() +
# theme_pubr() + stat_compare_means(method = "anova")  + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_P, !is.na(K_age_decay))$SUBJ)))
# 
# ggplot(dados_lobos_v1_P, aes(x = Diagnostic, y = logAvgThickness, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() +
# theme_pubr() +stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_P, !is.na(logAvgThickness))$SUBJ)))
# 
# ggplot(dados_lobos_v1_P, aes(x = Diagnostic, y = logAvgThickness_age_decay, color = Diagnostic, fill = Diagnostic)) +
# geom_violin(trim = FALSE, alpha = 0.4) + geom_jitter() +
# theme_pubr() + stat_compare_means(method = "anova") + labs(caption = paste("N = ", n_distinct(filter(dados_lobos_v1_P, !is.na(logAvgThickness_age_decay))$SUBJ)))

caret::featurePlot(x = train.data_lobes[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data_lobes$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

multinom1.l <- multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
multinom2.l <- multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)

multinom4.l <- multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
multinom5.l <- multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)

multinom0.l <- multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
multinom0_2.l <- multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)

## da estatistica ##
 
summary(multinom1.l)
summary(multinom2.l)
summary(multinom4.l)
summary(multinom5.l)
summary(multinom0.l)
summary(multinom0_2.l)

# anova(multinom5, multinom4, test = "Chisq")

## da estatistica ##

m.multi.nova1.l <-
  multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
  stargazer(m.multi.nova1.l, type = "text")

  z1.l <-
    summary(m.multi.nova1.l)$coefficients / summary(m.multi.nova1.l)$standard.errors
    p1.l <- (1 - pnorm(abs(z1.l), 0, 1)) * 2
    t(p1.l)

#Para facilitar a interpreta??o:
coef.multi1.l = exp(coef(m.multi.nova1.l))
t(coef.multi1.l)

#Previsoes
predicted.classes.multi.nova1.l <- m.multi.nova1.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova1.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova1.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova1.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)
  stargazer(m.multi.nova2.l, type = "text")

  z2.l <-
    summary(m.multi.nova2.l)$coefficients / summary(m.multi.nova2.l)$standard.errors
    p2.l <- (1 - pnorm(abs(z2.l), 0, 1)) * 2
    t(p2.l)

#Para facilitar a interpreta??o:
coef.multi2.l = exp(coef(m.multi.nova2.l))
t(coef.multi2.l)

#Previsoes
predicted.classes.multi.nova2.l <- m.multi.nova2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )
 
m.multi.nova4.l <-
  multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova4.l, type = "text")

  z4.l <-
    summary(m.multi.nova4.l)$coefficients / summary(m.multi.nova4.l)$standard.errors
    p4.l <- (1 - pnorm(abs(z4.l), 0, 1)) * 2
    t(p4.l)

#Para facilitar a interpreta??o:
coef.multi4.l = exp(coef(m.multi.nova4.l))
t(coef.multi4.l)

#Previsoes
predicted.classes.multi.nova4.l <- m.multi.nova4.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova4.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova4.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova4.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova5.l <-
  multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova5.l, type = "text")

  z5.l <-
    summary(m.multi.nova5.l)$coefficients / summary(m.multi.nova5.l)$standard.errors
    p5.l <- (1 - pnorm(abs(z5.l), 0, 1)) * 2
    t(p5.l)

#Para facilitar a interpreta??o:
coef.multi5.l = exp(coef(m.multi.nova5.l))
t(coef.multi5.l)

#Previsoes
predicted.classes.multi.nova5.l <- m.multi.nova5.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova5.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova5.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova5.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova0.l <-
  multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0.l, type = "text")

  z0.l <-
    summary(m.multi.nova0.l)$coefficients / summary(m.multi.nova0.l)$standard.errors
    p0.l <- (1 - pnorm(abs(z0.l), 0, 1)) * 2
    t(p0.l)

#Para facilitar a interpreta??o:
coef.multi0.l = exp(coef(m.multi.nova0.l))
t(coef.multi0.l)

#Previsoes
predicted.classes.multi.nova0.l <- m.multi.nova0.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0_2.l, type = "text")

  z0_2.l <-
    summary(m.multi.nova0_2.l)$coefficients / summary(m.multi.nova0_2.l)$standard.errors
    p0_2.l <- (1 - pnorm(abs(z0_2.l), 0, 1)) * 2
    t(p0_2.l)

#Para facilitar a interpreta??o:
coef.multi0_2.l = exp(coef(m.multi.nova0_2.l))
t(coef.multi0_2.l)

#Previsoes
predicted.classes.multi.nova0_2.l <- m.multi.nova0_2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0_2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```


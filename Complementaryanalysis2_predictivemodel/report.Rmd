---
title: "Complementary Analysis 1 from Fernanda Hansen P. de Moraes Thesis - Cortical folding alterations in humans due to aging and diseases"
author: "Fernanda Hansen Pacheco de Moraes"
date: "30-november-2021"
output:
  html_document: 
    df_print: kable
    fig_caption: yes
    fig_width: 8
    number_sections: yes
    theme: paper
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	warning = FALSE,
	messAge = FALSE
)
```


```{r}
setwd("~/GitHub/FHPdeMoraes_thesis/Complementaryanalysis2_predictivemodel")

```

```{r call packages}
library(readr)
library(tidyverse)
library(lubridate)
library(ggpubr)
library(kableExtra)
library(broom)
library(MASS)

# multinomial predictive model packages
require(foreign)
require(nnet)
require(reshape2)
library(stargazer)
require(betareg)
library(caret)
library(pROC)

```

```{r}
test_coef <- function(reg, coefnum, val){
  co <- coef(summary(reg))
  tstat <- (co[coefnum,1]-val)/co[coefnum,2]
  2 * pt(abs(tstat), reg$df.residual, lower.tail = FALSE)
}
```

```{r import files}
dados_raw <- read_csv("dados_raw.csv")
```

```{r create new variables}
# estimate cortical folding variables
dados_raw <- dados_raw %>%
  mutate( # create new variables
  localGI = TotalArea / ExposedArea,
  k = sqrt(AvgThickness) * TotalArea / (ExposedArea ^ 1.25),
  K = 1 / 4 * log10(AvgThickness^2)  + log10(TotalArea) - 5 / 4 * log10(ExposedArea),
  S = 3 / 2 * log10(TotalArea) + 3 / 4 * log10(ExposedArea) - 9 /  4 * log10(AvgThickness^2) ,
  I = log10(TotalArea) + log10(ExposedArea) + log10(AvgThickness^2),
  c = as.double(ifelse(ROI == "hemisphere", NA, 4 * pi / GaussianCurvature)),
  TotalArea_corrected = ifelse(ROI == "hemisphere", TotalArea, TotalArea * c),
  ExposedArea_corrected = ifelse(ROI == "hemisphere", ExposedArea, ExposedArea * c),
  logTotalArea_corrected = log10(TotalArea_corrected),
  logExposedArea_corrected = log10(ExposedArea_corrected),
  localGI_corrected = ifelse(
    ROI == "hemisphere",
    TotalArea / ExposedArea,
    TotalArea_corrected / ExposedArea_corrected
  ),
  k_corrected = ifelse(
    ROI == "hemisphere",
    sqrt(AvgThickness) * log10(TotalArea) / (log10(ExposedArea) ^ 1.25),
    sqrt(AvgThickness) * log10(TotalArea_corrected) / (log10(ExposedArea_corrected^1.25) )
  ),
  K_corrected =  ifelse(
    ROI == "hemisphere",
    1 / 4 * log10(AvgThickness^ 2)+ log10(TotalArea) - 5 / 4 * log10(ExposedArea),
    1 / 4 * log10(AvgThickness^ 2) + log10(TotalArea_corrected) - 5 / 4 * log10(ExposedArea_corrected)
  ),
  I_corrected = ifelse(
    ROI == "hemisphere",
    log10(TotalArea) + log10(ExposedArea) + log10(AvgThickness^ 2) ,
    log10(TotalArea_corrected) + log10(ExposedArea_corrected) + log10(AvgThickness^ 2) 
  ),
  S_corrected = ifelse(
    ROI == "hemisphere",
    3 / 2 * log10(TotalArea) + 3 / 4 * log10(ExposedArea) - 9 /  4 * log10(AvgThickness^ 2) ,
    3 / 2 * log10(TotalArea_corrected) + 3 / 4 * log10(ExposedArea_corrected) - 9 /  4 * log10(AvgThickness^ 2) 
  )
)

# create age intervals
dados_raw$Age_interval <- cut(dados_raw$Age,
                                       breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100),
                                       right = FALSE,
                                       include.lowest = TRUE)

dados_raw$Age_interval10 <- cut(dados_raw$Age,
                                         breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100),
                                         right = FALSE,
                                         include.lowest = TRUE)
```

```{r data cleanup}
dados_all <- dados_raw %>% filter(
    Diagnostic == "CONTROLE" |
      Diagnostic == "CCL" |
      Diagnostic == "ALZ", !is.na(logAvgThickness), ExposedArea != 0 | !is.na(localGI), !is.infinite(logExposedArea)) %>% 
  droplevels()

# excluded data
anti_join(dados_raw, dados_all) %>%
  kable() %>%
  kable_styling()

dados <- dados_all
```

```{r}
# rename diagnostics
dados$Diagnostic[dados$Diagnostic == "CONTROLE"] <- "CTL"
dados$Diagnostic[dados$Diagnostic == "ALZ"] <- "AD"
dados$Diagnostic[dados$Diagnostic == "CCL"] <- "MCI"
dados$Diagnostic <- factor(dados$Diagnostic, levels = c("AD", "MCI","CTL"))

# filter data
dados <- dados %>%
  filter(machine == "Philips-Achieva", # include only subjects acquired at Philips Achieva 3T
                          ESC == 8 | ESC > 8, # include only subjects with 8 years of scholarship or more
                          Session == 1) %>% # use only data from Session 1
  droplevels() # delete factor levels

```

```{r deaging}
# define age for deaging
Age.cor = 25

# AVG thickness ----
decay_AvgThickness <-
  filter(dados, Diagnostic == "CTL") %>%
  droplevels() %>%
  group_by(ROI) %>%
  do(fit_decay_AvgThickness = tidy(rlm(AvgThickness ~ Age, data = .), conf.int=TRUE)) %>% unnest(cols = c(fit_decay_AvgThickness)) %>%
  filter(term == "Age") %>%
    mutate(c_AvgThickness = estimate,
         std_error_c_AvgThickness = std.error) %>%
  dplyr::select(c(ROI, c_AvgThickness, std_error_c_AvgThickness))

## TotalArea ---
decay_TotalArea <- filter(dados, Diagnostic == "CTL", !is.na(TotalArea), !is.nan(TotalArea), !is.infinite(TotalArea)) %>%
  droplevels() %>%
  group_by(ROI) %>%
  do(fit_decay_TotalArea = tidy(rlm(TotalArea ~ Age, data = .),conf.int=TRUE)) %>%
  unnest(cols = c(fit_decay_TotalArea)) %>%
  filter(term == "Age") %>%
  mutate(c_TotalArea = estimate,
         std_error_c_TotalArea = std.error) %>%
  dplyr::select(c(ROI, c_TotalArea, std_error_c_TotalArea))

## ExposedArea ---
decay_ExposedArea <- filter(dados, Diagnostic == "CTL", !is.na(ExposedArea), !is.nan(ExposedArea), !is.infinite(ExposedArea)) %>%
  droplevels() %>%
  group_by(ROI) %>%
  do(fit_decay_ExposedArea = tidy(rlm(ExposedArea ~ Age, data = .), conf.int = TRUE)) %>%
  unnest(cols = c(fit_decay_ExposedArea)) %>%
  filter(term == "Age") %>%
  mutate(c_ExposedArea = estimate,
         std_error_c_ExposedArea = std.error) %>%
  dplyr::select(c(ROI, c_ExposedArea, std_error_c_ExposedArea))

dados <- full_join(dados, decay_AvgThickness) %>%
  full_join(decay_TotalArea) %>%
  full_join(decay_ExposedArea) %>%
  mutate(
    AvgThickness_age_decay = AvgThickness - c_AvgThickness * (Age - Age.cor),
    logAvgThickness_age_decay = log10(AvgThickness_age_decay),
    TotalArea_age_decay = TotalArea - c_TotalArea * (Age - Age.cor),
    logTotalArea_age_decay = log10(TotalArea_age_decay),
    ExposedArea_age_decay = ExposedArea - c_ExposedArea * (Age - Age.cor),
    logExposedArea_age_decay = log10(ExposedArea_age_decay),
    K_age_decay = log10(TotalArea_age_decay) + 1/4*log10(AvgThickness_age_decay^2) - 5/4*log10(ExposedArea_age_decay),
    I_age_decay = log10(TotalArea_age_decay) + log10(ExposedArea_age_decay) + log10(AvgThickness_age_decay^2),
    S_age_decay = 3/2*log10(TotalArea_age_decay) + 3/4*log10(ExposedArea_age_decay) - 9/4*log10(AvgThickness_age_decay^2))

dados$logAvgThickness <- as.double(dados$logAvgThickness)
dados$logExposedArea<- as.double(dados$logExposedArea)
dados$logTotalArea   <- as.double(dados$logTotalArea)

```

```{r}
dados_v1 <- filter(dados, ROI == "F" | ROI == "T" | ROI == "O" | ROI == "P" | ROI == "hemisphere") %>%
  droplevels()

# lobe data
dados_lobos_v1 <- unique(filter(dados, ROI == "F" | ROI == "T" | ROI == "O" | ROI == "P",  !is.na(K_age_decay), SUBJ != "SUBJ211", SUBJ != "SUBJ223")) %>%
  droplevels()

# hemisphere data
dados_hemi_v1 <- unique(filter(dados, ROI == "hemisphere", !is.na(K_age_decay)))
```

# Data description

### Somente Philips

```{r N sujeitos Philips, echo=FALSE, message=FALSE, warning=FALSE}
dados_hemi_v1 %>%
  group_by(Diagnostic) %>%
  summarise(
    N = n_distinct(SUBJ),
    age = paste(signif(mean(Age), 2), "±", signif(sd(Age), 2)),
    age_range = paste(signif(min(Age), 2), "; ", signif(max(Age), 2)),
    ESC = paste(signif(mean(ESC), 2), "±", signif(sd(ESC), 2)),
    T =  paste(signif(mean(AvgThickness), 2), "±", signif(sd(AvgThickness), 2)),
    AT =  paste(signif(mean(TotalArea), 2), "±", signif(sd(TotalArea), 2)),
    AE =  paste(signif(mean(ExposedArea), 2), "±", signif(sd(ExposedArea), 2)),
    k =  paste(signif(mean(k), 2), "±", signif(sd(k), 2)),
    K =  paste(signif(mean(K), 2), "±", signif(sd(K), 2)),
    S =  paste(signif(mean(S), 2), "±", signif(sd(S), 2)),
    I =  paste(signif(mean(I), 2), "±", signif(sd(I), 2))
  ) %>% kable(digits = 2) %>% kable_styling()

dados_hemi_v1 %>%
  group_by(Gender, Diagnostic) %>%
  summarise(
    N = n_distinct(SUBJ),
    age = paste(signif(mean(Age), 2), "±", signif(sd(Age), 2)),
    age_range = paste(signif(min(Age), 2), "; ", signif(max(Age), 2)),
    ESC = paste(signif(mean(ESC), 2), "±", signif(sd(ESC), 2)),
    T =  paste(signif(mean(AvgThickness), 2), "±", signif(sd(AvgThickness), 2)),
    AT =  paste(signif(mean(TotalArea), 2), "±", signif(sd(TotalArea), 2)),
    AE =  paste(signif(mean(ExposedArea), 2), "±", signif(sd(ExposedArea), 2)),
    k =  paste(signif(mean(k), 2), "±", signif(sd(k), 2)),
    K =  paste(signif(mean(K), 2), "±", signif(sd(K), 2)),
    S =  paste(signif(mean(S), 2), "±", signif(sd(S), 2)),
    I =  paste(signif(mean(I), 2), "±", signif(sd(I), 2))
  ) %>% kable(digits = 2) %>% kable_styling()

lm_Age <- lm(Age ~ Diagnostic, data = dados_hemi_v1)
anova(lm_Age)
Age_diag_tk <- TukeyHSD(aov(Age ~ Diagnostic, data = dados_hemi_v1))
Age_diag_tk

lm_esc <- lm(ESC ~ Diagnostic, data = dados_hemi_v1)
anova(lm_esc)
ESC_diag_TK <- TukeyHSD(aov(ESC ~ Diagnostic, data = dados_hemi_v1))
ESC_diag_TK

paste("N sujeitos = ", n_distinct(dados_hemi_v1$SUBJ))
paste("N sujeitos lobos = ", n_distinct(dados_lobos_v1$SUBJ))

SUBJ_hemi <- unique(dplyr::select(dados_hemi_v1, c(SUBJ)))
SUBJ_lobos <- unique(dplyr::select(dados_lobos_v1, c(SUBJ)))
anti_join(SUBJ_lobos, SUBJ_hemi)

```

```{r distribuicao da Sample}
ggplot(data = dados_hemi_v1, aes(x = Diagnostic, y = Age, color = Diagnostic, fill = Diagnostic, alpha = 0.4)) + 
  geom_boxplot() + theme_pubr() +
  stat_compare_means(method = "anova")


ggplot(data = dados_hemi_v1, aes(x = Diagnostic, y = K, color = Gender, fill = Gender, alpha = 0.4)) +
  geom_boxplot() + theme_pubr() +
  stat_compare_means(method = "anova")

```

# DIAGNOSTIC PREDICTION ----

## multinomial model

```{r glm, echo=TRUE, messAge=FALSE, warning=FALSE}

dados_hemi_v1$Diagnostic <- relevel(dados_hemi_v1$Diagnostic, "CTL")

set.seed(0)

N_diag <- dados_hemi_v1 %>% dplyr::select(SUBJ, Diagnostic) %>% unique() %>% group_by(Diagnostic) %>% summarise(n_DIAG = n_distinct(SUBJ))

dados_hemi_v1_filter <- dados_hemi_v1 %>% dplyr::select(SUBJ, Diagnostic) %>% unique()

N_CTL <- as.double(floor(N_diag[1,2]*0.8))
N_CCL <- as.double(round(N_diag[3,2]*0.8))
N_ALZ <- as.double(round(N_diag[2,2]*0.8))

test.samples <- c(sample(which(dados_hemi_v1_filter$Diagnostic == "AD"), N_ALZ), sample(which(dados_hemi_v1_filter$Diagnostic == "CTL"), N_CTL), sample(which(dados_hemi_v1_filter$Diagnostic == "MCI"), N_CCL))
subj.training <- as_tibble(dados_hemi_v1_filter[-test.samples, ]$SUBJ)

colnames(subj.training) <- c("SUBJ")

train.data <- anti_join(dados_hemi_v1, subj.training)
test.data <- semi_join(dados_hemi_v1, subj.training)

caret::featurePlot(x = dados_hemi_v1[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = dados_hemi_v1$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

caret::featurePlot(x = dados_hemi_v1[, c("K", "I", "S")], y = dados_hemi_v1$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(3,1))

caret::featurePlot(x = dados_hemi_v1[, c("K_age_decay", "I_age_decay", "S_age_decay")], y = dados_hemi_v1$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(3,1))

print(n_distinct(dados_hemi_v1$SUBJ))
print(n_distinct(train.data$SUBJ))
print(n_distinct(test.data$SUBJ))

caret::featurePlot(x = train.data[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

caret::featurePlot(x = train.data[, c("K", "K_age_decay", "I", "I_age_decay", "S", "S_age_decay")], y = train.data$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(3,2))

multinom1 <- multinom(Diagnostic ~ K + Age, data = train.data)
multinom2 <- multinom(Diagnostic ~ logAvgThickness + Age, data = train.data)

multinom10 <- multinom(Diagnostic ~ S + Age, data = train.data)
multinom11 <- multinom(Diagnostic ~ I + Age, data = train.data)

multinom0 <- multinom(Diagnostic ~ K + Age + ESC, data = train.data)
multinom0_2 <- multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data)

multinom_Gender1 <- multinom(Diagnostic ~ K + Age + Gender , data = train.data)
multinom_Gender2 <- multinom(Diagnostic ~ logAvgThickness + Age + Gender , data = train.data)

multinom0_0 <- multinom(Diagnostic ~ K + S + I + Age, data = train.data)

m.multi.nova1 <-
  multinom(Diagnostic ~ K + Age, data = train.data)
  stargazer(m.multi.nova1, type = "text")

  z1 <-
    summary(m.multi.nova1)$coefficients / summary(m.multi.nova1)$standard.errors
    p1 <- (1 - pnorm(abs(z1), 0, 1)) * 2
    t(p1)

#Para facilitar a interpreta??o:
coef.multi1 = exp(coef(m.multi.nova1))
t(coef.multi1)
#Previsoes
predicted.classes.multi.nova1 <- m.multi.nova1 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova1 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova1, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova1),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova2 <-
  multinom(Diagnostic ~ logAvgThickness + Age, data = train.data)
  stargazer(m.multi.nova2, type = "text")

  z2 <-
    summary(m.multi.nova2)$coefficients / summary(m.multi.nova2)$standard.errors
    p2 <- (1 - pnorm(abs(z2), 0, 1)) * 2
    t(p2)

#Para facilitar a interpreta??o:
coef.multi2 = exp(coef(m.multi.nova2))
t(coef.multi2)
#Previsoes
predicted.classes.multi.nova2 <- m.multi.nova2 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova2 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova2, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova2),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova10 <-
  multinom(Diagnostic ~ I + Age, data = train.data)
  stargazer(m.multi.nova10, type = "text")

  z10 <-
    summary(m.multi.nova10)$coefficients / summary(m.multi.nova10)$standard.errors
    p10 <- (1 - pnorm(abs(z10), 0, 1)) * 2
    t(p10)

#Para facilitar a interpreta??o:
coef.multi10 = exp(coef(m.multi.nova10))
t(coef.multi10)
#Previsoes
predicted.classes.multi.nova10 <- m.multi.nova10 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova10 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova10, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova10),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova11 <-
  multinom(Diagnostic ~ S + Age, data = train.data)
  stargazer(m.multi.nova10, type = "text")

  z11 <-
    summary(m.multi.nova11)$coefficients / summary(m.multi.nova11)$standard.errors
    p11 <- (1 - pnorm(abs(z11), 0, 1)) * 2
    t(p11)

#Para facilitar a interpreta??o:
coef.multi11 = exp(coef(m.multi.nova11))
t(coef.multi11)
#Previsoes
predicted.classes.multi.nova11 <- m.multi.nova11 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova11 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova11, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova11),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0 <-
  multinom(Diagnostic ~ K + Age + ESC, data = train.data)
  stargazer(m.multi.nova0, type = "text")
  
  z0 <-
    summary(m.multi.nova0)$coefficients / summary(m.multi.nova0)$standard.errors
    p0 <- (1 - pnorm(abs(z0), 0, 1)) * 2
    t(p0)

#Para facilitar a interpreta??o:
coef.multi0 = exp(coef(m.multi.nova0))
t(coef.multi0)
#Previsoes
predicted.classes.multi.nova0 <- m.multi.nova0 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova0, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_2 <-
  multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data)
  stargazer(m.multi.nova0_2, type = "text")

  z0_2 <-
    summary(m.multi.nova0_2)$coefficients / summary(m.multi.nova0_2)$standard.errors
    p0_2 <- (1 - pnorm(abs(z0_2), 0, 1)) * 2
    t(p0_2)

#Para facilitar a interpreta??o:
coef.multi0_2 = exp(coef(m.multi.nova0_2))
t(coef.multi0_2)
#Previsoes
predicted.classes.multi.nova0_2 <- m.multi.nova0_2 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_2 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova0_2, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_2),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_0 <-
  multinom(Diagnostic ~ K + I + S + Age, data = train.data)
  stargazer(m.multi.nova0_0, type = "text")

  z0_0 <-
    (summary(m.multi.nova0_0)$coefficients)/(summary(m.multi.nova0_0)$standard.errors)
    p0_0 <- (1 - pnorm(abs(z0_0), 0, 1)) * 2
    t(p0_0)

#Para facilitar a interpreta??o:
coef.multi0_0 = exp(coef(m.multi.nova0_0))
t(coef.multi0_0)
#Previsoes
predicted.classes.multi.nova0_0 <- m.multi.nova0_0 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_0 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0_0, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_0),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "0_0-Specificity (false positive rate)"
  )


m.multi.nova.Gender1 <-
  multinom(Diagnostic ~ K + Age + Gender, data = train.data)
  stargazer(m.multi.nova.Gender1, type = "text")

  z.Gender1 <-
    summary(m.multi.nova.Gender1)$coefficients / summary(m.multi.nova.Gender1)$standard.errors
    p.Gender1 <- (1 - pnorm(abs(z.Gender1), 0, 1)) * 2
    t(p.Gender1)

#Para facilitar a interpreta??o:
coef.multi.Gender1 = exp(coef(m.multi.nova.Gender1))
t(coef.multi.Gender1)
#Previsoes
predicted.classes.multi.nova.Gender1 <- m.multi.nova.Gender1 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender1 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender1, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender1),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova.Gender2 <-
  multinom(Diagnostic ~ logAvgThickness_age_decay + Age + Gender, data = train.data)
  stargazer(m.multi.nova.Gender2, type = "text")

  z.Gender2 <-
    summary(m.multi.nova.Gender2)$coefficients / summary(m.multi.nova.Gender2)$standard.errors
    p.Gender2 <- (1 - pnorm(abs(z.Gender2), 0, 1)) * 2
    t(p.Gender2)

#Para facilitar a interpreta??o:
coef.multi.Gender2 = exp(coef(m.multi.nova.Gender2))
t(coef.multi.Gender2)
#Previsoes
predicted.classes.multi.nova.Gender2 <- m.multi.nova.Gender2 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender2 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender2, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender2),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

## modelo multinomial - deAged

```{r glm deAged, echo=TRUE, messAge=FALSE, warning=FALSE}

multinom4 <- multinom(Diagnostic ~ K_age_decay, data = train.data)
multinom5 <- multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data)

multinom12 <- multinom(Diagnostic ~ I_age_decay + logAvgThickness_age_decay, data = train.data)
multinom13 <- multinom(Diagnostic ~ S_age_decay + logAvgThickness_age_decay, data = train.data)

multinom0_0_0 <- multinom(Diagnostic ~ K_age_decay + I_age_decay + S_age_decay, data = train.data)

## da estatistica ##
 
m.multi.nova4 <-
  multinom(Diagnostic ~ K_age_decay, data = train.data)
  stargazer(m.multi.nova4, type = "text")

  z4 <-
    summary(m.multi.nova4)$coefficients / summary(m.multi.nova4)$standard.errors
    p4 <- (1 - pnorm(abs(z4), 0, 1)) * 2
    t(p4)

#Para facilitar a interpreta??o:
coef.multi4 = exp(coef(m.multi.nova4))
t(coef.multi4)
#Previsoes
predicted.classes.multi.nova4 <- m.multi.nova4 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova4 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova4, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova4),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova5 <-
  multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data)
  stargazer(m.multi.nova5, type = "text")

  z5 <-
    summary(m.multi.nova5)$coefficients / summary(m.multi.nova5)$standard.errors
    p5 <- (1 - pnorm(abs(z5), 0, 1)) * 2
    t(p5)

#Para facilitar a interpreta??o:
coef.multi5 = exp(coef(m.multi.nova5))
t(coef.multi5)
#Previsoes
predicted.classes.multi.nova5 <- m.multi.nova5 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova5 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova5, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova5),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova12 <-
  multinom(Diagnostic ~ I_age_decay, data = train.data)
  stargazer(m.multi.nova12, type = "text")

  z12 <-
    summary(m.multi.nova12)$coefficients / summary(m.multi.nova12)$standard.errors
    p12 <- (1 - pnorm(abs(z12), 0, 1)) * 2
    t(p12)

#Para facilitar a interpreta??o:
coef.multi12 = exp(coef(m.multi.nova12))
t(coef.multi12)
#Previsoes
predicted.classes.multi.nova12 <- m.multi.nova12 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova12 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova12, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova12),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova13 <-
  multinom(Diagnostic ~ S_age_decay, data = train.data)
  stargazer(m.multi.nova12, type = "text")

  z13 <-
    summary(m.multi.nova13)$coefficients / summary(m.multi.nova13)$standard.errors
    p13 <- (1 - pnorm(abs(z13), 0, 1)) * 2
    t(p13)

#Para facilitar a interpreta??o:
coef.multi13 = exp(coef(m.multi.nova13))
t(coef.multi13)
#Previsoes
predicted.classes.multi.nova13 <- m.multi.nova13 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova13 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova13, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova13),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )
  

m.multi.nova0_0_0 <-
  multinom(Diagnostic ~ K_age_decay + I_age_decay + S_age_decay, data = train.data)
  stargazer(m.multi.nova0_0_0, type = "text")

  z0_0_0 <-
    summary(m.multi.nova0_0_0)$coefficients / summary(m.multi.nova0_0_0)$standard.errors
    p0_0_0 <- (1 - pnorm(abs(z0_0_0), 0, 1)) * 2
    t(p0_0_0)

#Para facilitar a interpreta??o:
coef.multi0_0_0 = exp(coef(m.multi.nova0_0_0))
t(coef.multi0_0_0)
#Previsoes
predicted.classes.multi.nova0_0_0 <- m.multi.nova0_0_0 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_0_0 == test.data$Diagnostic)


# Summary
confusionMatrix(predicted.classes.multi.nova0_0_0, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_0_0),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )  
  

m.multi.nova.Gender3 <-
  multinom(Diagnostic ~ K_age_decay + Gender, data = train.data)
  stargazer(m.multi.nova.Gender3, type = "text")

  z.Gender3 <-
    summary(m.multi.nova.Gender3)$coefficients / summary(m.multi.nova.Gender3)$standard.errors
    p.Gender3 <- (1 - pnorm(abs(z.Gender3), 0, 1)) * 2
    t(p.Gender3)

#Para facilitar a interpreta??o:
coef.multi.Gender3 = exp(coef(m.multi.nova.Gender3))
t(coef.multi.Gender3)
#Previsoes
predicted.classes.multi.nova.Gender3 <- m.multi.nova.Gender3 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender3 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender3, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender3),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova.Gender4 <-
  multinom(Diagnostic ~ logAvgThickness + Age + Gender, data = train.data)
  stargazer(m.multi.nova.Gender4, type = "text")

  z.Gender4 <-
    summary(m.multi.nova.Gender4)$coefficients / summary(m.multi.nova.Gender4)$standard.errors
    p.Gender4 <- (1 - pnorm(abs(z.Gender4), 0, 1)) * 2
    t(p.Gender4)

#Para facilitar a interpreta??o:
coef.multi.Gender4 = exp(coef(m.multi.nova.Gender4))
t(coef.multi.Gender4)
#Previsoes
predicted.classes.multi.nova.Gender4 <- m.multi.nova.Gender4 %>% predict(test.data, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova.Gender4 == test.data$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova.Gender4, test.data$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data$Diagnostic),
  as.numeric(predicted.classes.multi.nova.Gender4),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = FALSE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

## modelo multinomial - lobes

### Lobo temporal

```{r glm temporal lobe, echo=TRUE, messAge=FALSE, warning=FALSE}

dados_lobos_v1_T <- filter(dados_lobos_v1, ROI == "T")

dados_lobos_v1_T$Diagnostic <- relevel(dados_lobos_v1_T$Diagnostic, "CTL")

train.data_lobes <- anti_join(dados_lobos_v1_T, subj.training)
test.data_lobes <- semi_join(dados_lobos_v1_T, subj.training)

caret::featurePlot(x = dados_lobos_v1_T[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = dados_lobos_v1_T$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

print(n_distinct(dados_lobos_v1_T$SUBJ))
print(n_distinct(train.data_lobes$SUBJ))
print(n_distinct(test.data_lobes$SUBJ))

caret::featurePlot(x = train.data_lobes[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data_lobes$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

multinom1.l <- multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
multinom2.l <- multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)

multinom4.l <- multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
multinom5.l <- multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)

multinom0.l <- multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
multinom0_2.l <- multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)

## da estatistica ##
 
summary(multinom1.l)
summary(multinom2.l)
summary(multinom4.l)
summary(multinom5.l)
summary(multinom0.l)
summary(multinom0_2.l)

# anova(multinom5, multinom4, test = "Chisq")

## da estatistica ##

m.multi.nova1.l <-
  multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
  stargazer(m.multi.nova1.l, type = "text")

  z1.l <-
    summary(m.multi.nova1.l)$coefficients / summary(m.multi.nova1.l)$standard.errors
    p1.l <- (1 - pnorm(abs(z1.l), 0, 1)) * 2
    t(p1.l)

#Para facilitar a interpreta??o:
coef.multi1.l = exp(coef(m.multi.nova1.l))
t(coef.multi1.l)

#Previsoes
predicted.classes.multi.nova1.l <- m.multi.nova1.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova1.l == test.data_lobes$Diagnostic)

# Summary
cM1.l <- confusionMatrix(predicted.classes.multi.nova1.l, test.data_lobes$Diagnostic)
cM1.l
#cM1.l.t.score <- mutate(cM1.l, Diagnostic = c("AD", "MCI", "CTL"),sensitivity = as.data.frame(cM1.l$byClass[,1]), specificity = as.data.frame(cM1.l$byClass[,2]))

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova1.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)
  stargazer(m.multi.nova2.l, type = "text")

  z2.l <-
    summary(m.multi.nova2.l)$coefficients / summary(m.multi.nova2.l)$standard.errors
    p2.l <- (1 - pnorm(abs(z2.l), 0, 1)) * 2
    t(p2.l)

#Para facilitar a interpreta??o:
coef.multi2.l = exp(coef(m.multi.nova2.l))
t(coef.multi2.l)

#Previsoes
predicted.classes.multi.nova2.l <- m.multi.nova2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )
 
m.multi.nova4.l <-
  multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova4.l, type = "text")

  z4.l <-
    summary(m.multi.nova4.l)$coefficients / summary(m.multi.nova4.l)$standard.errors
    p4.l <- (1 - pnorm(abs(z4.l), 0, 1)) * 2
    t(p4.l)

#Para facilitar a interpreta??o:
coef.multi4.l = exp(coef(m.multi.nova4.l))
t(coef.multi4.l)

#Previsoes
predicted.classes.multi.nova4.l <- m.multi.nova4.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova4.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova4.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova4.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova5.l <-
  multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova5.l, type = "text")

  z5.l <-
    summary(m.multi.nova5.l)$coefficients / summary(m.multi.nova5.l)$standard.errors
    p5.l <- (1 - pnorm(abs(z5.l), 0, 1)) * 2
    t(p5.l)

#Para facilitar a interpreta??o:
coef.multi5.l = exp(coef(m.multi.nova5.l))
t(coef.multi5.l)

#Previsoes
predicted.classes.multi.nova5.l <- m.multi.nova5.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova5.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova5.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova5.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova0.l <-
  multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0.l, type = "text")

  z0.l <-
    summary(m.multi.nova0.l)$coefficients / summary(m.multi.nova0.l)$standard.errors
    p0.l <- (1 - pnorm(abs(z0.l), 0, 1)) * 2
    t(p0.l)

#Para facilitar a interpreta??o:
coef.multi0.l = exp(coef(m.multi.nova0.l))
t(coef.multi0.l)

#Previsoes
predicted.classes.multi.nova0.l <- m.multi.nova0.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0_2.l, type = "text")

  z0_2.l <-
    summary(m.multi.nova0_2.l)$coefficients / summary(m.multi.nova0_2.l)$standard.errors
    p0_2.l <- (1 - pnorm(abs(z0_2.l), 0, 1)) * 2
    t(p0_2.l)

#Para facilitar a interpreta??o:
coef.multi0_2.l = exp(coef(m.multi.nova0_2.l))
t(coef.multi0_2.l)

#Previsoes
predicted.classes.multi.nova0_2.l <- m.multi.nova0_2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0_2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

### Lobo parietal

```{r glm parietal lobe, echo=TRUE, messAge=FALSE, warning=FALSE}

dados_lobos_v1_P <- filter(dados_lobos_v1, ROI == "P")

dados_lobos_v1_P$Diagnostic <- factor(dados_lobos_v1_P$Diagnostic, levels = c("AD", "MCI","CTL"))

train.data_lobes <- anti_join(dados_lobos_v1_P, subj.training)
test.data_lobes <- semi_join(dados_lobos_v1_P, subj.training)

caret::featurePlot(x = dados_lobos_v1_P[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = dados_lobos_v1_P$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

print(n_distinct(dados_lobos_v1_P$SUBJ))
print(n_distinct(train.data_lobes$SUBJ))
print(n_distinct(test.data_lobes$SUBJ))

caret::featurePlot(x = train.data_lobes[, c("K", "logAvgThickness", "K_age_decay", "logAvgThickness_age_decay")], y = train.data_lobes$Diagnostic, plot = "box", scales = list(y = list(relation = "free"), x = list(rot = 90)), layout = c(4, 1))

multinom1.l <- multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
multinom2.l <- multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)

multinom4.l <- multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
multinom5.l <- multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)

multinom0.l <- multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
multinom0_2.l <- multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)

## da estatistica ##
 
summary(multinom1.l)
summary(multinom2.l)
summary(multinom4.l)
summary(multinom5.l)
summary(multinom0.l)
summary(multinom0_2.l)

# anova(multinom5, multinom4, test = "Chisq")

## da estatistica ##

m.multi.nova1.l <-
  multinom(Diagnostic ~ K_corrected + Age, data = train.data_lobes)
  stargazer(m.multi.nova1.l, type = "text")

  z1.l <-
    summary(m.multi.nova1.l)$coefficients / summary(m.multi.nova1.l)$standard.errors
    p1.l <- (1 - pnorm(abs(z1.l), 0, 1)) * 2
    t(p1.l)

#Para facilitar a interpreta??o:
coef.multi1.l = exp(coef(m.multi.nova1.l))
t(coef.multi1.l)

#Previsoes
predicted.classes.multi.nova1.l <- m.multi.nova1.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova1.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova1.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova1.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age, data = train.data_lobes)
  stargazer(m.multi.nova2.l, type = "text")

  z2.l <-
    summary(m.multi.nova2.l)$coefficients / summary(m.multi.nova2.l)$standard.errors
    p2.l <- (1 - pnorm(abs(z2.l), 0, 1)) * 2
    t(p2.l)

#Para facilitar a interpreta??o:
coef.multi2.l = exp(coef(m.multi.nova2.l))
t(coef.multi2.l)

#Previsoes
predicted.classes.multi.nova2.l <- m.multi.nova2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )
 
m.multi.nova4.l <-
  multinom(Diagnostic ~ K_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova4.l, type = "text")

  z4.l <-
    summary(m.multi.nova4.l)$coefficients / summary(m.multi.nova4.l)$standard.errors
    p4.l <- (1 - pnorm(abs(z4.l), 0, 1)) * 2
    t(p4.l)

#Para facilitar a interpreta??o:
coef.multi4.l = exp(coef(m.multi.nova4.l))
t(coef.multi4.l)

#Previsoes
predicted.classes.multi.nova4.l <- m.multi.nova4.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova4.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova4.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova4.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova5.l <-
  multinom(Diagnostic ~ logAvgThickness_age_decay, data = train.data_lobes)
  stargazer(m.multi.nova5.l, type = "text")

  z5.l <-
    summary(m.multi.nova5.l)$coefficients / summary(m.multi.nova5.l)$standard.errors
    p5.l <- (1 - pnorm(abs(z5.l), 0, 1)) * 2
    t(p5.l)

#Para facilitar a interpreta??o:
coef.multi5.l = exp(coef(m.multi.nova5.l))
t(coef.multi5.l)

#Previsoes
predicted.classes.multi.nova5.l <- m.multi.nova5.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova5.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova5.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova5.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )


m.multi.nova0.l <-
  multinom(Diagnostic ~ K_corrected + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0.l, type = "text")

  z0.l <-
    summary(m.multi.nova0.l)$coefficients / summary(m.multi.nova0.l)$standard.errors
    p0.l <- (1 - pnorm(abs(z0.l), 0, 1)) * 2
    t(p0.l)

#Para facilitar a interpreta??o:
coef.multi0.l = exp(coef(m.multi.nova0.l))
t(coef.multi0.l)

#Previsoes
predicted.classes.multi.nova0.l <- m.multi.nova0.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

m.multi.nova0_2.l <-
  multinom(Diagnostic ~ logAvgThickness + Age + ESC, data = train.data_lobes)
  stargazer(m.multi.nova0_2.l, type = "text")

  z0_2.l <-
    summary(m.multi.nova0_2.l)$coefficients / summary(m.multi.nova0_2.l)$standard.errors
    p0_2.l <- (1 - pnorm(abs(z0_2.l), 0, 1)) * 2
    t(p0_2.l)

#Para facilitar a interpreta??o:
coef.multi0_2.l = exp(coef(m.multi.nova0_2.l))
t(coef.multi0_2.l)

#Previsoes
predicted.classes.multi.nova0_2.l <- m.multi.nova0_2.l %>% predict(test.data_lobes, type = "class")

#Model accuracy
mean(predicted.classes.multi.nova0_2.l == test.data_lobes$Diagnostic)

# Summary
confusionMatrix(predicted.classes.multi.nova0_2.l, test.data_lobes$Diagnostic)

#ROC
multiclass.roc(
  as.numeric(test.data_lobes$Diagnostic),
  as.numeric(predicted.classes.multi.nova0_2.l),
  percent = F,
  ci.alpha = 0.9,
  stratified = FALSE,
  plot = TRUE,
  grid = TRUE,
  legacy.axes = TRUE,
  reuse.auc = TRUE,
  print.auc = TRUE,
  print.thres.col = "blue",
  ci.type = "bars",
  print.thres.cex = 0.7,
  main = "ROC curve",
  ylab = "Sensitivity (true positive rate)",
  xlab = "1-Specificity (false positive rate)"
  )

```

